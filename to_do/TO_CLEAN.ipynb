{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b86ca6a",
   "metadata": {},
   "source": [
    "these need to be added to the package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab97397",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372dbf04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "import sys\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import mimetypes\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38535191-9708-4fca-b4a5-5b436e70adbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f021e77-5d14-41e6-a5ed-bf89363c2c11",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fc8ef-9737-483e-8770-0bb84ed9a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_directory = home_directory + \"config/\"\n",
    "\n",
    "sys.path.append(config_directory)  # Adjust 'config_directory' as necessary\n",
    "from passwords import password_CDIS_PROD  # Ensure 'passwords' is accessible in the configuration directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fec2eb-427a-46d8-b55d-0840429e9a23",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67914714-aa49-4271-bacf-be69fe32d1a7",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392b518-28d7-4d3a-80d9-56ed773d12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def graph_basic_histogram(data, x, xlabel='', xlimit=None, output_name='basic_histogram', bins=100, color=grey, hue=None, palette=None, label_map=None):\n",
    "    \n",
    "#     # limit x-axis\n",
    "#     if xlimit != None:\n",
    "#         data = data[data[x] <= xlimit]\n",
    "        \n",
    "#     # GRAPH\n",
    "#     plt.figure(figsize=(figsize_width, figsize_height))\n",
    "#     plot = sns.histplot(data=data, x=x, hue=hue, alpha=1, edgecolor='white', color=color, bins=bins, palette=palette)\n",
    "    \n",
    "\n",
    "#     formatting(plot, xlabel=xlabel, y_grid=True, numeric_y=True, numeric_x=True)\n",
    "#     export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d125a-74d7-4446-8b5b-3fd9e913c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_hist_annex_counts(data, x, output_name='hist_annex_counts', xlabel=None, datalabels=True):\n",
    "\n",
    "    # GRAPH\n",
    "    plt.figure(figsize=(figsize_width, figsize_height))\n",
    "    grouped_data = data.groupby([x, 'B_MOTIVATION_IN_ANNEX']).size().unstack(fill_value=0)\n",
    "    plot = grouped_data.plot(kind='bar', stacked=True, alpha=1, edgecolor='none', color=[dict_B_MOTIVATION_IN_ANNEX_palette.get(x, \"#333333\") for x in grouped_data.columns])\n",
    "    \n",
    "    # FORMATTING\n",
    "    plt.xticks(rotation=0)\n",
    "    formatting(plot, xlabel=xlabel, y_grid=True, numeric_y=True)\n",
    "    if datalabels:\n",
    "        format_datalabels(plot, label_offset=0.015)\n",
    "    format_legend(label_map=dict_B_MOTIVATION_IN_ANNEX_mapping, ncol=10, bbox_to_anchor=(0, 1.05))\n",
    "    \n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1746fda4-0b4c-4bf6-a97b-d73b38ae0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_hist_token_length(data, x, hue, xlabel='', output_name='hist_token_length', bins=100, palette=['#dadfe6',B_MOTIVATION_IN_ANNEX_color], x_limit=None, dict_hue=dict_B_MOTIVATION_IN_ANNEX_mapping):\n",
    " \n",
    "    if x_limit is not None:\n",
    "        data = data[data[x] < x_limit]\n",
    "    # GRAPH\n",
    "    plt.figure(figsize=(figsize_width, figsize_height))\n",
    "    plot = sns.histplot(data=data, x=x, hue=hue, alpha=1, multiple=\"stack\", edgecolor='white', bins=bins, palette=palette)\n",
    "\n",
    "    # FORMATTING\n",
    "    formatting(plot, xlabel=xlabel, y_grid=True, numeric_x=True, numeric_y=True)\n",
    "    if dict_hue is not None:\n",
    "        format_legend(dict_hue, B_incorrect_legend=True)\n",
    "    else:\n",
    "        if hasattr(plot, 'legend_') and plot.legend_ is not None:\n",
    "            plot.legend_.remove()\n",
    "    \n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23443a43-7788-495f-81fc-af7810d6ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_hist_character_length(data, hue, palette, label_map, output_name='hist_character_length_complaints', x='T_MOTIVATION', B_incorrect_legend=True, limit=8000):\n",
    "    \n",
    "    # CALCULATE CHARACTER LENGTH\n",
    "    df = data.copy()\n",
    "    df['Character Length'] = df[x].apply(lambda x: len(str(x))).copy()\n",
    "    df = df[df['Character Length'] < limit]\n",
    "    \n",
    "    # GRAPH\n",
    "    plot = sns.histplot(data=df, x='Character Length', hue=hue, multiple=\"stack\", kde=False, alpha=1, bins=100, palette=palette)\n",
    "\n",
    "    # FORMATTING\n",
    "    formatting(plot, xlabel='Character Length', y_offset=0.97, y_labelpad=15, x_grid=True, y_grid=True, numeric_y=True, numeric_x=True)\n",
    "    format_legend(label_map, ncol=10, bbox_to_anchor=(0, 1.07), B_incorrect_legend=True)\n",
    "    \n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad832c-c828-4736-b697-153676927115",
   "metadata": {},
   "source": [
    "## Lineplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64002bf6-0e26-432a-b9d2-522f2d2816f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def graph_plot_training_history(history, metric='loss', ylabel='Loss'):\n",
    "    \n",
    "#     # GET DATA\n",
    "#     metric_values = history.history[metric]\n",
    "#     val_metric_values = history.history[f'val_{metric}']\n",
    "#     epochs = range(1, len(metric_values) + 1)\n",
    "\n",
    "#     # GRAPH\n",
    "#     plt.figure(figsize=(figsize_width, figsize_height))\n",
    "#     plot = plt.plot(epochs, metric_values, label=f'Training {metric}', color=grey)\n",
    "#     plot = plt.plot(epochs, val_metric_values, label=f'Validation {metric}', color='black')\n",
    "    \n",
    "#     # FORMATTING\n",
    "#     format_legend(label_map=None, bbox_to_anchor=(0, 1.07))\n",
    "#     formatting(plot, xlabel='Epochs', ylabel=ylabel, y_offset=0.97, y_labelpad=15, x_grid=False, y_grid=False)\n",
    "        \n",
    "#     export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27318912-477c-4fa4-92bb-14d6065e3ad5",
   "metadata": {},
   "source": [
    "## Countplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7540bff-d3a5-46b5-9c3d-d64c73ae9542",
   "metadata": {},
   "source": [
    "### Vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59419f7e-7a3a-46fe-84c5-1f0bbb83fefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graph_basic_countplot_x(data, x, output_name='basic_countplot', xlabel='', numeric_x=False, numeric_y=True, ordered=False):\n",
    "    \n",
    "    order = data[x].value_counts().index if ordered else None\n",
    "\n",
    "    # GRAPH\n",
    "    plt.figure(figsize=(figsize_width, figsize_height))\n",
    "    plot = sns.countplot(data=data, x=x, alpha=1, edgecolor='none', color=grey, order=order)\n",
    "\n",
    "    # FORMATTING    \n",
    "    formatting(plot, xlabel=xlabel, y_grid=True, numeric_x=numeric_x, numeric_y=numeric_y)\n",
    "    format_datalabels(plot,label_offset=0.007)\n",
    "\n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19c03582-3485-4069-9547-2cdd8a507de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graph_document_count_distribution(data, x, hue, output_name='document_count_distribution'):\n",
    "    \n",
    "    # GRAPH\n",
    "    plt.figure(figsize=(figsize_width/2, figsize_height))\n",
    "    plot = sns.countplot(data=data, x=x, hue=hue, alpha=1, saturation=1, edgecolor='none', palette=dict_B_MOTIVATION_IN_ANNEX_palette)\n",
    "    \n",
    "    # FORMATTING\n",
    "    formatting(plot, xlabel='B_MOTIVATION_IN_ANNEX', y_grid=True, numeric_y=True)\n",
    "    format_legend(label_map=dict_B_MOTIVATION_IN_ANNEX_mapping, ncol=1, bbox_to_anchor=(0, 1.08))\n",
    "    format_datalabels(plot)\n",
    "    \n",
    "    x_axis_mapping = {0: 'No Documents in Annex',\n",
    "                     1: '1 Document in Annex'}\n",
    "    plt.xticks(ticks=list(x_axis_mapping.keys()), labels=list(x_axis_mapping.values()))\n",
    "    \n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8163160f-acd0-4286-8b02-af762f9b35b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graph_file_types_distribution(data, x, xlabel=None, output_name='file_types_distribution'):\n",
    "        \n",
    "    order = data[x].value_counts().index\n",
    "\n",
    "    # GRAPH\n",
    "    plt.figure(figsize=(figsize_width, figsize_height))\n",
    "    plot = sns.countplot(data=data, x=x, alpha=1, saturation=1, edgecolor='none', order=order, color=B_MOTIVATION_IN_ANNEX_color)\n",
    "    \n",
    "    # FORMATTING\n",
    "    formatting(plot, xlabel=xlabel, y_grid=True, numeric_y=True)\n",
    "    format_datalabels(plot,label_offset=0.007)\n",
    "    \n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0b8af06-9eaf-4233-8e3d-f2aa9264bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_fiscal_year_hist(data, x, output_name='fiscal_year_hist', xlabel=''):\n",
    "\n",
    "    # GRAPH\n",
    "    plt.figure(figsize=(figsize_width, figsize_height))\n",
    "    plot = sns.countplot(data=data, x=x, alpha=1, edgecolor='none', color=grey)\n",
    "\n",
    "    # FORMATTING    \n",
    "    formatting(plot, xlabel=xlabel, y_grid=True, numeric_x=True, numeric_y=True)\n",
    "    format_datalabels(plot,label_offset=0.007)\n",
    "\n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80976f22-2639-4f74-96e4-2b32197f4821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def distribution_complaints_by_label(data, x, figsize_width=figsize_width/3, figsize_height=figsize_height/2, output_name='distribution_complaints_by_label'):\n",
    "    \n",
    "    # SORTING VALUES\n",
    "    order = data[x].value_counts(ascending=False).index\n",
    " \n",
    "    # CHART\n",
    "    plt.figure(figsize=(figsize_width, figsize_height))\n",
    "    plot = sns.countplot(data=data, x=x, alpha=1, saturation=1, color='#723746', order=order, edgecolor='None', width=0.5)\n",
    "\n",
    "    # FORMATTING\n",
    "    formatting(plot, xlabel='', x_labelpad=10, y_offset=0.93, y_labelpad=10, y_grid=True, numeric_y=False)\n",
    "    plt.xticks(ticks=[0, 1], labels=list_contentious_labels)\n",
    "    \n",
    "    # DATALABELS\n",
    "    total = len(data[x]) \n",
    "    for p in plot.patches:\n",
    "        percentage = '{:.0f}%'.format(100 * p.get_height() / total)\n",
    "        x = p.get_x() + p.get_width() / 2\n",
    "        y = p.get_y() + p.get_height() / 2  # text in the middle of the bar\n",
    "        plot.annotate(percentage, (x, y), ha='center', va='center', fontsize=fontsize_datalabels, color='white')\n",
    "      \n",
    "    # SHOWING PLOT AND EXPORTING\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8288f9-4623-450d-9335-367bdb460568",
   "metadata": {},
   "source": [
    "### Horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca78a897-0791-4ade-9d08-62f346b100a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graph_distribution_by_motivation_in_annex(data, y, target, hue, output_name='distribution_by_motivation_in_annex'):\n",
    "    \n",
    "    order = data[y].value_counts().index\n",
    "    # counts = data[hue].value_counts()\n",
    "    \n",
    "    # GRAPH\n",
    "    plt.figure(figsize=(figsize_width*0.75,figsize_height))\n",
    "    plot = sns.countplot(data=data, y=y, hue=hue, palette=dict_B_MOTIVATION_IN_ANNEX_palette, alpha=1, saturation=1, edgecolor='none', order=order, dodge=True)\n",
    "\n",
    "    # FORMATTING\n",
    "    formatting(plot, ylabel='Inputchannel', y_offset=0.92, y_labelpad=0, x_grid=True, numeric_x=True)\n",
    "    format_legend(dict_B_MOTIVATION_IN_ANNEX_mapping, ncol=2, bbox_to_anchor=(0, 1.06))\n",
    "    format_datalabels(plot, orientation='horizontal')\n",
    "\n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0970762d-ee50-41d3-b826-dce3202c81a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def griefs_by_contentious(data, y, hue, output_name='griefs_by_contentious', legend=True, margin=100, ylabel='Level 1 Griefs', ypad=0.93, labelpad=0, dodge=False, sort='0'):\n",
    "    # SORTING griefs_by_contentious\n",
    "    if sort == '0':\n",
    "        order = data[y].value_counts(ascending=False).index[:50]\n",
    "    else:\n",
    "        sorted_data = data.sort_values(by=sort, ascending=False)\n",
    "        order = sorted_data[y].unique()\n",
    "\n",
    "    # CHART\n",
    "    plt.figure(figsize=(figsize_width, figsize_height))\n",
    "    plot = sns.countplot(data=data, y=y, hue=hue, alpha=1, saturation=1, order=order, palette=dict_non_contentious_palette, dodge=dodge)\n",
    "\n",
    "    # FORMATTING\n",
    "    formatting(plot, ylabel=ylabel, y_offset=ypad, y_labelpad=labelpad, x_grid=True, numeric_x=True)\n",
    "    format_datalabels(plot,label_offset=0.007, orientation='horizontal') \n",
    "      \n",
    "    # LEGEND\n",
    "    if legend == True:\n",
    "        format_legend(dict_label_map_contentious, ncol=10, bbox_to_anchor=(-0.01, 1.06), B_incorrect_legend=True)\n",
    "    else:\n",
    "        if plot.legend_:\n",
    "            plot.legend_.remove()\n",
    "    \n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556b288-2a8f-4192-a4af-d5a747a2df03",
   "metadata": {},
   "source": [
    "## Barplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3eae0e-cae9-4512-a2d2-001a85b41ed2",
   "metadata": {},
   "source": [
    "### Horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c42691c3-aafd-4945-be13-94818544846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_barplot_horizontal(data, x, y, hue=None, palette=None, output_name='barplot_horizontal', ylabel='', y_offset=0.95, formatting_type='totals', figsize_height=figsize_height, figsize_width=figsize_width, legendpad=1.09, legend_ncol=1, legend_label_map=None):\n",
    "    \n",
    "    data = data.copy()\n",
    "    \n",
    "    if figsize_height==True:\n",
    "        figsize_height = (len(data[y].value_counts()) / 2) + 1\n",
    "        \n",
    "    if legend_label_map is not None:\n",
    "        data[hue] = data[hue].map(legend_label_map)\n",
    "    \n",
    "    # GRAPH\n",
    "    plt.figure(figsize=(figsize_width, figsize_height))\n",
    "    if hue==None:\n",
    "        plot = sns.barplot(data=data, x=x, y=y, hue=hue, alpha=1, saturation=1, edgecolor='none', color=grey)\n",
    "    else:\n",
    "        plot = sns.barplot(data=data, x=x, y=y, hue=hue, alpha=1, saturation=1, edgecolor='none', palette=palette)\n",
    "\n",
    "    # FORMATTING\n",
    "    formatting(plot, xlabel=x, x_labelpad=10, ylabel=ylabel, y_labelpad=15, x_grid=True, numeric_x=True, y_offset=y_offset)\n",
    "    if hue!=None:\n",
    "        format_legend(label_map=None, ncol=legend_ncol, bbox_to_anchor=(0, legendpad))\n",
    "    format_datalabels(plot, formatting=formatting_type, orientation='horizontal')\n",
    " \n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12365271-23c8-431a-beb4-9c9434db62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_accuracy_by_MOTIVATION_IN_ANNEX(data, x, y, hue, output_name='accuracy_by_MOTIVATION_IN_ANNEX'):\n",
    "    \n",
    "    # GRAPH\n",
    "    plt.figure(figsize=(figsize_width/2, figsize_height))\n",
    "    plot = sns.barplot(data=data, x=x, y=y, hue=hue, alpha=1, saturation=1, edgecolor='none', palette=['#7d8691','#dadfe6'])\n",
    "\n",
    "    # FORMATTING\n",
    "    formatting(plot, xlabel=x, x_labelpad=15, ylabel='Accuracy', y_labelpad=15, y_grid=True, numeric_y=True)\n",
    "    format_legend(label_map=None, ncol=1, bbox_to_anchor=(0, 1.09))\n",
    "    format_datalabels(plot, formatting='percentage')\n",
    "    \n",
    "    x_axis_mapping = {0: 'Motivation in Finaliti',\n",
    "                     1: 'Motivation in Annex'}\n",
    "    plt.xticks(ticks=list(x_axis_mapping.keys()), labels=list(x_axis_mapping.values()))\n",
    " \n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45722f-1113-4b47-8f1b-82a05a24808b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stacked Bar Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cbf786-789c-4f7b-a44a-69dc86901bd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b246887-cb1c-4e1f-906f-0a885d6dc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_accuracy_top_5(data, output_name, B_only_first_and_5=False):\n",
    "    # PIVOT DATA\n",
    "    df = data.rename(columns={'percentage_correct': 'Correctly Classified', 'percentage_misclassified': 'Misclassified'})\n",
    "    if B_only_first_and_5:\n",
    "        df = df[df['top_n'].str.contains('1|5')]\n",
    "    \n",
    "    # GRAPH\n",
    "    if B_only_first_and_5:\n",
    "        plt.figure(figsize=(figsize_width/2.5, figsize_height))\n",
    "    else:\n",
    "        plt.figure(figsize=(figsize_width, figsize_height))\n",
    "    ax = plt.gca()\n",
    "    plot = df.plot(kind='bar', stacked=True, color=[green, red], edgecolor='none', ax=plt.gca(), alpha=1, width=0.5)\n",
    "\n",
    "    # FORMATTING\n",
    "    plt.yticks(np.arange(0,101,10))\n",
    "    ax.set_xticklabels(df['top_n'].values, rotation=0)\n",
    "    formatting(plot, xlabel='Total Predictions Provided', ylabel='Percentage', y_offset=0.94, y_labelpad=10, y_grid=True)\n",
    "    format_legend(label_map=None, ncol=10, bbox_to_anchor=(0, 1.05))\n",
    "\n",
    "\n",
    "            \n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0ea5a20-21ec-4f58-85f8-8a9402528b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graph_distribution_augmented_data(y, df, hue='augmented_data', output_name='distribution_augmented_data'):\n",
    "    \n",
    "    # PIVOT DATA\n",
    "    grouped_data = df.groupby([y, hue]).size().reset_index(name='counts')\n",
    "    pivot_data = grouped_data.pivot(index=y, columns=hue, values='counts').fillna(0)\n",
    "    total_counts = pivot_data[0] # This will select the column for non-augmented data (0 is the code for non-augmented)\n",
    "    order = total_counts.sort_values(ascending=False).index  # Changed sorting to descending for vertical display\n",
    " \n",
    "    # COLORMAPPING\n",
    "    palette = ['#723746',pink, pink]\n",
    "\n",
    "    # GRAPH\n",
    "    plt.figure(figsize=(figsize_width, figsize_height))\n",
    "    plot = pivot_data.loc[order].plot(kind='bar', stacked=True, ax=plt.gca(), alpha=1, edgecolor='none', width=0.7, color=palette)\n",
    "\n",
    "    # FORMATTING\n",
    "    formatting(plot, xlabel='Griefs', y_offset=1, y_labelpad=0, y_grid=True, numeric_y=True)\n",
    "    plt.xticks([])\n",
    "    format_legend({0: 'Original Data', 1: 'Added Data'}, ncol=10, bbox_to_anchor=(-0.008, 1.07))\n",
    "    \n",
    "    # EXPORT\n",
    "    export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68abaed-b7dd-4219-8e84-1bf21eed4001",
   "metadata": {},
   "source": [
    "### Horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade0361-d2d7-4d02-8841-9bff95e42c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # OVERVIEW MISCLASSIFICATION\n",
    "\n",
    "# def graph_overview_misclassification(data, y, hue, output_name='overview_misclassification', top='', ntop=15, nbottom=30):\n",
    "#     # STACKED BAR CHART (HORIZONTAL)\n",
    "#     plt.figure(figsize=(figsize_width*0.8, figsize_height*1.2))    \n",
    "#     pivot_data = data.groupby([y, hue]).size().unstack().fillna(0)\n",
    "#     ordered_index = pivot_data.sum(axis=1).sort_values(ascending=True).index\n",
    "    \n",
    "#     # TOP OR BOTTOM VALUES\n",
    "#     if top=='top':\n",
    "#         pivot_data = pivot_data.reindex(ordered_index[-ntop:])\n",
    "#     elif top=='bottom':\n",
    "#         pivot_data = pivot_data.reindex(ordered_index[:nbottom])\n",
    "#     else:\n",
    "#         pivot_data = pivot_data.reindex(ordered_index)\n",
    "    \n",
    "#     # GRAPH\n",
    "#     plot = pivot_data.plot(kind='barh', stacked=True, ax=plt.gca(), alpha=1, edgecolor='none', width=0.7, color=[dict_palette_good_bad_True_False[col] for col in pivot_data.columns])\n",
    "\n",
    "#     # FORMATTING\n",
    "#     plt.yticks(fontsize=fontsize_ticks-1)  \n",
    "#     formatting(plot, ylabel='', x_grid=True, numeric_x=True)\n",
    "       \n",
    "#     # FORMATTING LEGEND\n",
    "#     counts = data[hue].value_counts()\n",
    "    \n",
    "#     legend_patches = [\n",
    "#         Patch(color=red, label='Misclassified: {:,}'.format(counts.get(False, 0)).replace(',','.')),\n",
    "#         Patch(color=green, label='Correctly Classified: {:,}'.format(counts.get(True, 0)).replace(',','.'))\n",
    "#     ]\n",
    "#     plt.legend(handles=legend_patches, title='', bbox_to_anchor=(-0.01, 1.05), loc='upper left', ncol=10, handletextpad=0.5, framealpha=0.0, fontsize=fontsize_legend)\n",
    "\n",
    "#     # DATALABELS\n",
    "#     max_count = pivot_data.sum(axis=1).max()\n",
    "#     threshold = 0.04 * max_count\n",
    "    \n",
    "#     for index, (value1, value2) in enumerate(pivot_data.values):\n",
    "#         # CALCULATING THE PERCENTAGE\n",
    "#         total = value1 + value2\n",
    "#         value1_percentage = (value1 / total) * 100\n",
    "#         value2_percentage = (value2 / total) * 100\n",
    "        \n",
    "#         # FORMATTING THE LABELS\n",
    "#         if value1 >= threshold: #only plot if the bar is visible\n",
    "#             plot.text(value1 / 2, index, f\"{value1_percentage:.0f}%\", ha='center', va='center', color='white', fontsize=fontsize_datalabels-1)\n",
    "#         if value2 >= threshold: #only plot if the bar is visible\n",
    "#             plot.text(value1 + value2 / 2, index, f\"{value2_percentage:.0f}%\", ha='center', va='center', color='black', fontsize=fontsize_datalabels-1)\n",
    "    \n",
    "#     # EXPORT\n",
    "#     export_graphs(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4869203-93a3-4cc5-bc01-a318ef53bd26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b12cc6-d908-4d80-90ca-8c21602cc824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def confusion_matrix(df, actual_col, predicted_col, positive_class):\n",
    "    # Get unique classes\n",
    "    classes = sorted(set(df[actual_col].unique()) | set(df[predicted_col].unique()))\n",
    "    \n",
    "    # Reorder classes to put positive class first\n",
    "    classes.remove(positive_class)\n",
    "    classes.insert(0, positive_class)\n",
    "    \n",
    "    # Create confusion matrix with raw counts\n",
    "    cm = pd.crosstab(df[actual_col], df[predicted_col], dropna=False)\n",
    "    \n",
    "    # Ensure all expected classes are present\n",
    "    for cls in classes:\n",
    "        if cls not in cm.index:\n",
    "            cm.loc[cls] = 0\n",
    "        if cls not in cm.columns:\n",
    "            cm[cls] = 0\n",
    "    \n",
    "    # Reorder to ensure positive class is first\n",
    "    cm = cm.reindex(index=classes, columns=classes)\n",
    "    \n",
    "    # Calculate total sum of all elements\n",
    "    total = cm.values.sum()\n",
    "    \n",
    "    # Normalize to get percentages based on the total sum\n",
    "    cm_percentage = cm / total * 100\n",
    "    \n",
    "    # Round to integers for readability\n",
    "    cm_percentage = cm_percentage.round(0).astype(int)\n",
    "    \n",
    "    # Transpose: Predicted on the left, Actual on the top\n",
    "    cm_percentage = cm_percentage.T\n",
    "    \n",
    "    # Compute metrics from raw counts\n",
    "    total_samples = len(df)\n",
    "    correct_predictions = np.trace(cm.values)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    \n",
    "    # Precision: TP / (TP + FP)\n",
    "    tp = cm.loc[positive_class, positive_class]\n",
    "    fp = cm.loc[:, positive_class].sum() - tp\n",
    "    precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Recall: TP / (TP + FN)\n",
    "    total_actual = cm.loc[positive_class, :].sum()\n",
    "    fn = total_actual - tp\n",
    "    recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.0f}%\")\n",
    "    print(f\"Precision: {precision:.0f}%\")\n",
    "    print(f\"Recall: {recall:.0f}%\")\n",
    "    \n",
    "    return cm_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185997f-d0cc-4a20-a535-e8cc37046ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CDIS data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a853e-b9ef-44ee-a89f-f8961e0c6546",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extracting text from documents\n",
    "- read in the contents of the pdf, page by page\n",
    "- by each UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "610efe93-1476-4c7a-9a66-f1aff4e3e60e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PDF contents\n",
    "def extract_pdf_contents(response_content):\n",
    "    doc = fitz.open(\"pdf\", response_content.content)\n",
    "    text_output = text_output = [page.get_text() for page in doc]\n",
    "    doc.close()\n",
    "    return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6afe496f-3de2-4475-9d3d-d30e2aa8a8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OCR\n",
    "def serialize_image(image):\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    image.save(img_byte_arr, format='JPEG')\n",
    "    return img_byte_arr.getvalue()\n",
    "\n",
    "def deserialize_image(image_bytes):\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image, max_size=2000):\n",
    "    image = image.convert('L') # Convert to grayscale\n",
    "    image = image.filter(ImageFilter.SHARPEN) # Apply image filters to enhance the text\n",
    "    # Resize image if it's too large\n",
    "    if image.width > max_size or image.height > max_size:\n",
    "        scaling_factor = max(image.width / max_size, image.height / max_size)\n",
    "        new_width = int(image.width / scaling_factor)\n",
    "        new_height = int(image.height / scaling_factor)\n",
    "        image = image.resize((new_width, new_height), Image.BILINEAR)\n",
    "    return image\n",
    "\n",
    "def ocr_image(image_bytes):\n",
    "    image = deserialize_image(image_bytes)\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "def extract_text_with_ocr_optimized(response_content):\n",
    "    images = pdf2image.convert_from_bytes(response_content.content)\n",
    "    preprocessed_images = [preprocess_image(image) for image in images]  # Preprocess each image\n",
    "    serialized_images = [serialize_image(image) for image in preprocessed_images]  # Serialize preprocessed images\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        text_output = list(executor.map(ocr_image, serialized_images))\n",
    "    \n",
    "    return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a60d5d97-a2b4-4eda-9d1f-b8e2e9425a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WORD DOCUMENTS\n",
    "def extract_text_from_docx(response_content):\n",
    "    docx_bytes = response_content.content\n",
    "    docx_stream = io.BytesIO(docx_bytes)\n",
    "    doc = docx.Document(docx_stream)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    text = '\\n'.join(full_text)\n",
    "    return [text] # Convert to list for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d5a571-6aee-4984-a9ba-018257fe3058",
   "metadata": {},
   "source": [
    "### API call function\n",
    "- process the data by filetype\n",
    "- 1 at a time (not for dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41ce0dd4-e612-4d03-ae81-288fadc8d430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def CDIS_API_call(CDIS_UUID, filetype, function, list_valid_file_types, username='fait', password=password_CDIS_PROD, type_db='PROD', save=False):\n",
    "#     # Define the URL prefix based on the database type\n",
    "#     url_prefixes = {\n",
    "#         'DEV': 'http://cdis.docmgmttest.finbel.intra:4080/document/',\n",
    "#         'ACC': 'http://cdis.docmgmtacc.finbel.intra:4080/document/',\n",
    "#         'PROD': 'http://cdis.docmgmt.finbel.intra:4080/document/'\n",
    "#     }\n",
    "#     url_prefix = url_prefixes.get(type_db)\n",
    "#     # Generate endpoint URLs\n",
    "#     url_content = f\"{url_prefix}{CDIS_UUID}/content\"\n",
    "#     url_info = f\"{url_prefix}{CDIS_UUID}/info\"\n",
    "#     try:\n",
    "#         # API call CONTENT\n",
    "#         response_content = THIS I WANT A FUNCTION requests.get(url_content, auth=HTTPBasicAuth(username, password))\n",
    "        \n",
    "#         # Check if the response is successful\n",
    "#         if response_content.status_code == 200:\n",
    "#             if filetype in list_valid_file_types:\n",
    "#                 if function is not None:\n",
    "#                     text = function(response_content)\n",
    "#             else:\n",
    "#                 return None  # Return None for unsupported file types\n",
    "            \n",
    "#             # Optionally save the content to a file (only for debugging)\n",
    "#             if save:\n",
    "#                 file_ext = '.' + filetype.lower()\n",
    "#                 with open(f'{CDIS_UUID}{file_ext}', 'wb') as file:\n",
    "#                     file.write(response_content.content)\n",
    "#             if function is not None:\n",
    "#                 return text  # Return the extracted text\n",
    "#         return None  # Return None if status is not 200\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"An exception occurred: {e}\")\n",
    "#         return None  # Return None in case of an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52caad9e-1297-4d7a-b761-52505a00c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CDIS_API_call(CDIS_UUID, save_path='', save_file=True, username='fait', password=password_CDIS_PROD, type_db='PROD'):\n",
    "    url_prefixes = {\n",
    "        'DEV': 'http://cdis.docmgmttest.finbel.intra:4080/document/',\n",
    "        'ACC': 'http://cdis.docmgmtacc.finbel.intra:4080/document/',\n",
    "        'PROD': 'http://cdis.docmgmt.finbel.intra:4080/document/'\n",
    "    }\n",
    "    url_prefix = url_prefixes.get(type_db)\n",
    "    url_content = f\"{url_prefix}{CDIS_UUID}/content\"\n",
    "    \n",
    "    try:\n",
    "        response_content = requests.get(url_content, auth=HTTPBasicAuth(username, password))\n",
    "        \n",
    "        if response_content.status_code == 200:\n",
    "            if save_file:\n",
    "                content_type = response_content.headers.get('Content-Type', '')\n",
    "                file_ext = mimetypes.guess_extension(content_type) or ''\n",
    "                file_path = os.path.join(save_path, f'{CDIS_UUID}{file_ext}')\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response_content.content)\n",
    "            return response_content.content\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b5e2e00-7e1a-4dca-934c-5714ff7e2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_CDIS_content(CDIS_UUID, filetype, function, list_valid_file_types, username, password, type_db):\n",
    "    if filetype not in list_valid_file_types:\n",
    "        return None  # Return None for unsupported file types\n",
    "    \n",
    "    try:\n",
    "        # Get the content directly from the API call\n",
    "        content = CDIS_API_call(CDIS_UUID, save_file=False)\n",
    "        \n",
    "        if content is None:\n",
    "            return None  # Return None if API call failed\n",
    "        \n",
    "        # Process the content\n",
    "        if function is not None:\n",
    "            text = function(content)\n",
    "            return text  # Return the extracted text\n",
    "        return None  # Return None if no function is provided\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred while processing the content: {e}\")\n",
    "        return None  # Return None in case of an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a3ad0-74e4-4937-b54b-9f9de871fa71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining in data to process\n",
    "- DATAFRAME PROCESSING\n",
    "- only call the API to data that hasnt been called before\n",
    "- limit to how many rows to be processed in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06d2d77a-a1e7-4f27-8a5a-582aa75c9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def ensure_csv_exists(loc, list_cols):\n",
    "    if not os.path.exists(loc):\n",
    "        # Create an empty CSV file with headers\n",
    "        with open(loc, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(list_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "632feceb-b843-4e9e-a500-23ee7d1662dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_processed_data(loc, df_all_ids, list_cols):\n",
    "    # Ensure the CSV file exists before trying to read it\n",
    "    ensure_csv_exists(loc, list_cols)\n",
    "    \n",
    "    # Read in the existing CSV data\n",
    "    df_csv = pd.read_csv(loc)\n",
    "    \n",
    "    # Handle the case where df_csv is empty\n",
    "    if df_csv.empty:\n",
    "        print(\"No previously processed documents. Processing all IDs.\")\n",
    "        return df_all_ids[:LIMIT_ROWS]  # All IDs need to be processed\n",
    "    \n",
    "    print(f\"Previously processed documents: {len(df_csv)}\")\n",
    "    # Merge: get all CDIS_UUIDs that haven't been processed yet\n",
    "    df_all = pd.merge(df_all_ids, df_csv[['CDIS_UUID', 'CDIS_extraction_type']], how='left', on='CDIS_UUID')\n",
    "    \n",
    "    # only keep data that hasn't been processed yet\n",
    "    df = df_all[df_all['CDIS_extraction_type'].isna()]\n",
    "    df = df[:LIMIT_ROWS]\n",
    "    print(f\"Documents currently processing: {len(df)}\")\n",
    "    \n",
    "    # Return the DataFrame with unprocessed IDs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d23c4d-5330-4861-940e-58999ba6b81d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calling each row in the dataframe \n",
    "- DATAFRAME PROCESSING\n",
    "- To prevent overloading the CDIS API: processing in batches with a delay of 5 seconds\n",
    "- each row gets saved to the VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a34b9cbd-0a22-416b-9a3e-30f87ea8acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_row_to_VM(row, save_location, column_order):\n",
    "    \n",
    "    # Create a new dictionary with only the required columns in the correct order\n",
    "    ordered_row = {col: row.get(col, '') for col in column_order}\n",
    "    \n",
    "    # Create a Series from the ordered dictionary\n",
    "    row_series = pd.Series(ordered_row)\n",
    "    \n",
    "    # Drop the 'file_type' column if it exists\n",
    "    row_series = row_series.drop('file_type', errors='ignore')\n",
    "    \n",
    "    # Create a DataFrame with the ordered row\n",
    "    df_row = pd.DataFrame([row_series])\n",
    "    \n",
    "    # Write to CSV, ensuring the correct order of columns\n",
    "    df_row.to_csv(save_location, mode='a', header=False, index=False, columns=column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ad38af0-6ed3-436e-ba2a-b917f8a8fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, function, loc, list_cols, tokenizer, list_valid_file_types):\n",
    "    # Call process_CDIS_content for the row\n",
    "    if B_troubleshooting:\n",
    "        print(row['CDIS_UUID'])\n",
    "    content = process_CDIS_content(row['CDIS_UUID'], row['file_type'], function, list_valid_file_types)\n",
    "    if content:\n",
    "        row['contents_original'] = content\n",
    "        row['n_pages'] = len(content) if isinstance(content, list) else 1\n",
    "        row['contents'] = '\\n'.join(content) if isinstance(content, list) else content\n",
    "        if row['contents'] is not None and row['contents'].strip():\n",
    "            row['CDIS_extraction_type'] = str(function.__name__)  # Using the function name for type\n",
    "            row['n_tokens'] = calculate_token_length(row['contents'], tokenizer)      \n",
    "            save_row_to_VM(row, loc, list_cols)\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f146a147-8531-49c2-81f3-b074d299ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row_not_successful(row, df, index, loc, list_cols):\n",
    "    row['CDIS_extraction_type'] = 'FAILED'  # Update row's type directly\n",
    "    df.at[index, 'CDIS_extraction_type'] = 'FAILED'  # Ensure DataFrame is updated I DONT KNOW WHY THIS IS NEEDED\n",
    "    save_row_to_VM(row, loc, list_cols)  # Save the row with 'FAILED' status   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0dec73-92f5-4054-aad3-ae44629179ff",
   "metadata": {},
   "source": [
    "### Calculate Token Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85db6e54-9dd1-4af9-baa9-0a9e41747b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/environments/shirin/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "def calculate_token_length(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "486c4f18-df3a-45a1-a630-08dbbcc8e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_length(row, col, tokenizer):\n",
    "    text = row[col]\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    \n",
    "    # Transform text into string if it's a list\n",
    "    if isinstance(text, list):\n",
    "        text = '\\n'.join(text)\n",
    "    \n",
    "    # Ensure text is a string\n",
    "    text = str(text)\n",
    "    \n",
    "    return calculate_token_length(text, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4792fe63-7de0-43d9-a374-f1ab7ee93db1",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "\n",
    "- DATAFRAME PROCESSING\n",
    "- read in the contents of the pdf, page by page\n",
    "- by each UUID\n",
    "- in batches to prevent overloading the CDIS API\n",
    "- each row gets saved to the VM (so when the VM crashes, progress is not lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecfa935a-2515-4a20-898f-15dc8095a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def API_calls_to_csv(loc, df_all_ids, list_valid_file_types):\n",
    "    \n",
    "    list_cols = ['CDIS_UUID', 'CDIS_extraction_type', 'n_pages', 'n_tokens', 'contents', 'contents_original']\n",
    "    # only keep data that hasn't been processed yet\n",
    "    df = read_in_processed_data(loc=loc, df_all_ids=df_all_ids, list_cols=list_cols)\n",
    "    \n",
    "    # initiliaze tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "    # iterate\n",
    "    for i in range(0, len(df), BATCH_SIZE):\n",
    "        batch = df.iloc[i:i+BATCH_SIZE]\n",
    "        for index, row in batch.iterrows():\n",
    "            \n",
    "            # pdf\n",
    "            if row['file_type'] == 'pdf' and ('CDIS_extraction_type' not in df.columns or pd.isna(row['CDIS_extraction_type'])):\n",
    "                # First try PDF extraction\n",
    "                successful = process_row(row, extract_pdf_contents, loc, list_cols, tokenizer, list_valid_file_types)\n",
    "                # If PDF extraction fails, try OCR\n",
    "                if not successful:\n",
    "                    successful = process_row(row, extract_text_with_ocr_optimized, loc, list_cols, tokenizer, list_valid_file_types)\n",
    "                # If OCR also fails, mark as FAILED\n",
    "                if not successful:\n",
    "                    process_row_not_successful(row, df, index, loc, list_cols)\n",
    "                    \n",
    "            # word docs      \n",
    "            elif row['file_type'] in list_word_docs and pd.isna(row['CDIS_extraction_type']):\n",
    "                successful = process_row(row, extract_text_from_docx, loc, tokenizer, list_valid_file_types)\n",
    "                if not successful:\n",
    "                    process_row_not_successful(row, df, index, loc, list_cols)\n",
    "            \n",
    "                    \n",
    "        time.sleep(5)  # Throttle the requests\n",
    "        if B_troubleshooting:\n",
    "            print(f\"Batch {i} successful\")\n",
    "        \n",
    "    # Load the saved file to return the final combined DataFrame\n",
    "    return pd.read_csv(loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "User kernel - Shirin",
   "language": "python",
   "name": "shirin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
